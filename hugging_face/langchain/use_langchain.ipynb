{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using langchain with a local LLM or API-Based\n",
    "\n",
    "See also\n",
    "https://colab.research.google.com/drive/1h2505J5H4Y9vngzPD08ppf1ga8sWxLvZ?usp=sharing#scrollTo=GMg2xiRnfm21"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = 'google/flan-t5-large'# go for a smaller model if you dont have the VRAM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### playing around with the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play or have picnic. 7\n",
      "running run but probably one at most to have great exercise. 14\n",
      "hike 3\n",
      "exercise 3\n",
      "workout 3\n",
      "exercise run first in light headed gusts like yesterday. 13\n",
      "hike or running/riding 8\n",
      "hiking 3\n",
      "run after school. But it gets windier during the run as each lap passes by in an amazing array of motion. Sometimes it is much closer than any aural music to music â€” that, being itself at times, provides 50\n",
      "swim? 4\n",
      "0    tensor(0)    <pad>\n",
      "1    tensor(9728)    swim\n",
      "2    tensor(58)    ?\n",
      "3    tensor(1)    </s>\n"
     ]
    }
   ],
   "source": [
    "input_context = \"The weather is really nice today. I'm thinking about going for a\"\n",
    "input_ids = tokenizer.encode(input_context, return_tensors=\"pt\")\n",
    "for i in range(10):\n",
    "    output = model.generate(input_ids, max_length=50, do_sample=True, temperature=2.0)\n",
    "    print(tokenizer.decode(output.flatten(), skip_special_tokens=True), len(output.flatten()))\n",
    "    \n",
    "res = output.flatten()\n",
    "for i,t in enumerate(res):\n",
    "    print(i, '  ', t, '  ', tokenizer.decode(t))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using langchain prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Question: What is the capital of England?\n",
      "Let's think step by step. Provide the answer to the question in the following way:\n",
      "Answer:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The capital of England is London. London is the capital of England. The answer: London.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Let's think step by step. Provide the answer to the question in the following way:\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, \n",
    "                     llm=llm,\n",
    "                     verbose=True\n",
    "                     )\n",
    "\n",
    "question = \"What is the capital of England?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Question: Oliver has 10 tennis balls. He loses 1 ball every 2 weeks, after 4 weeks he buys 2 packs each having 2 balls. How many balls does he have after 8 weeks? \n",
      "Let's think step by step. Provide the answer to the question in the following way:\n",
      "Answer:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Oliver loses 1 ball every 2 weeks so he loses 1 * 2 = 2 balls every 2 weeks. He buys 2 packs of tennis balls so he buys 2 * 2 = 4 packs of tennis balls. He loses 1 ball every 2 weeks so he loses 1 * 2 = 2 balls every 2 weeks. He buys 4 packs of tennis balls so he buys 4 * 4 = 32 balls. He has 32 balls and loses 1 ball every'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.run(\"Oliver has 10 tennis balls. He loses 1 ball every 2 weeks, after 4 weeks he buys 2 packs each having 2 balls. How many balls does he have after 8 weeks? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
